# -*- coding: utf-8 -*-
"""Classificação de ECG - 2.ipynb

Automatically generated by Colaboratory.


"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from keras.layers import Dense, Convolution1D, MaxPool1D, Flatten, Dropout
from keras.layers import Input
from keras.models import Model
import keras
from tensorflow.keras.layers import BatchNormalization
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.utils.np_utils import to_categorical

X_train = pd.read_excel('')
X_test = pd.read_excel('')

Y_train = pd.read_csv('', header=None)
Y_test = pd.read_csv('',header=None)

"""Verificando o tamanho das amostras.

Descrição: Every row is a QRS Complex.

Labels IDs:

0 = Normal

1 = Supraventricular ectopic beat

2 = Ventricular ectopic beat

3 = Fusion Beat

4 = Unknown

"""

print('shape do X_train: ' + str(X_train.shape)) 
print('shape do Y_train: ' + str(Y_train.shape)) 
print('shape do X_test: ' + str(X_test.shape)) 
print('shape do Y_test: ' + str(Y_test.shape)) 
print('\n')
print('Quantidade de classes no Y_train: \n' + str(Y_train.value_counts()))
print('\n')
print('Quantidade de classes no Y_test: \n' + str(Y_test.value_counts()))

"""Vamos plotar um sinal para vermos a forma dele"""

sinal = X_train.iloc[0].values
x = np.arange(len(sinal))

left = 0 #anormal_index[1] - 1080
right = len(sinal) #anormal_index[1] + 1080 # 1 segundo equivale a 360 hz

plt.plot(x[left:right], sinal[left:right],'-', label='ECG')
#plt.plot(x[peaks], p_signal_teste[peaks,0],'go', label='Pico')

plt.xlim(left,right)
plt.ylim(sinal[left:right].min()-0.05, sinal[left:right].max()+0.05)
plt.xlabel('Time index - Hz no tempo')
plt.ylabel('ECG signal')
plt.legend(bbox_to_anchor = (1.04,1), loc = 'upper left')
plt.rcParams["figure.figsize"] = (25,5)
plt.show()

"""Definição de funções para criar o modelo"""

#Queremos pegar poacotes de 180 valores, que é justamento o complexo QRS, para isso, usamos o reshape do numpy, 
#para transformar o dataframe em uma sequência de 51002 arrays, cada um desses arrays vão ter os 180 valores, somente com 1 coluna.
X_train = X_train.values.reshape(len(X_train), X_train.shape[1],1)
X_train.shape

X_test = X_test.values.reshape(len(X_test), X_test.shape[1],1)
X_test.shape

Y_train = to_categorical(Y_train)
Y_test = to_categorical(Y_test)

Y_train[0]

def network(X_train,Y_train,X_test,Y_test):
    

    im_shape=(X_train.shape[1],1)
    inputs_cnn=Input(shape=(im_shape), name='inputs_cnn')
    conv1_1=Convolution1D(64, (6), activation='relu', input_shape=im_shape)(inputs_cnn)
    conv1_1=BatchNormalization()(conv1_1) # Batch normalization is another form of regularization that rescales the outputs of a layer to make sure that they have mean 0 and standard deviation 1.
    pool1=MaxPool1D(pool_size=(3), strides=(2), padding="same")(conv1_1) # maxpooling é para diminuir a quantidade de parâmetros nos feature maps, já que ele seleciona somente os maiores valores, é como se fosse uam convolução que só seleciona os maiores valores
    conv2_1=Convolution1D(64, (3), activation='relu', input_shape=im_shape)(pool1)
    conv2_1=BatchNormalization()(conv2_1)
    pool2=MaxPool1D(pool_size=(2), strides=(2), padding="same")(conv2_1)
    conv3_1=Convolution1D(64, (3), activation='relu', input_shape=im_shape)(pool2)
    conv3_1=BatchNormalization()(conv3_1)
    pool3=MaxPool1D(pool_size=(2), strides=(2), padding="same")(conv3_1)
    flatten=Flatten()(pool3)
    dense_end1 = Dense(64, activation='relu')(flatten)
    dense_end2 = Dense(32, activation='relu')(dense_end1)
    main_output = Dense(4, activation='softmax', name='main_output')(dense_end2) #adicionar uma camada densa para classificar entre as 4 classes
    
    
    model = Model(inputs= inputs_cnn, outputs=main_output)
    model.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'])
    
    
    callbacks = [EarlyStopping(monitor='val_loss', patience=8, mode='min'), 
             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)] #usamos o callback 'ModelCheckpoint para pegar os melhores valores de parâmetros antes da rede começar a ter overfitting

    history=model.fit(X_train, Y_train,epochs=40,callbacks=callbacks, batch_size=32,validation_data=(X_test,Y_test))
    model.load_weights('best_model.h5') #Após os callbacks salvarem os melhores pesos, nós tremos que iniciar o modelo com os pesos salvos.
    return(model,history)

def evaluate_model(history,X_test,Y_test,model):
    scores = model.evaluate((X_test),Y_test, verbose=0)
    print("Accuracy: %.2f%%" % (scores[1]*100))
    
    print(history)
    fig1, ax_acc = plt.subplots(figsize=(15,15))
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.xlabel('Época', fontsize=18)
    plt.ylabel('Acurácia', fontsize=18)
    plt.title('Modelo - Acurácia', fontsize=20)
    ax_acc.legend(['Treinamento', 'Validação'], loc='lower right', prop={'size': 20})
    plt.show()
    
    fig2, ax_loss = plt.subplots(figsize=(15,15))
    plt.xlabel('Época', fontsize=18)
    plt.ylabel('Função custo', fontsize=18)
    plt.title('Modelo- Loss', fontsize=20)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    ax_loss.legend(['Treinamento', 'Validação'], loc='upper right', prop={'size': 20})
    plt.show()
    target_names=['0','1','2','3','4']
    
    #y_true=[]
    #for element in Y_test:
        #y_true.append(np.argmax(element))
    #prediction_proba=model.predict(X_test)
    #prediction=np.argmax(prediction_proba,axis=1)

from sklearn.metrics import confusion_matrix
import itertools
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title, fontsize=20)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('Classes Reais', fontsize=18)
    plt.xlabel('Classes Preditas', fontsize=18)

model,history=network(X_train,Y_train,X_test,Y_test)

evaluate_model(history,X_test,Y_test,model)

"""Fazendo a previsão nos dados de teste"""

y_pred=model.predict(X_test)

y_pred

# Compute confusion matrix
cnf_matrix = confusion_matrix(Y_test.argmax(axis=1), y_pred.argmax(axis=1))
np.set_printoptions(precision=2)

# Plot non-normalized confusion matrix
plt.figure(figsize=(15, 15))
plot_confusion_matrix(cnf_matrix, classes=['N', 'S', 'V', 'F'],normalize=False,
                      title='Matriz de confusão')
plt.rcParams.update({'font.size': 30})
plt.show()

"""Vamos plotar a matriz de confusão

Temos que fazer o y_pred retornar variáveis categóricas assim como o Y_test, para podermos fazer comparações. Para isso, vamos usar uma função que computa de acordo com a probabilidade encontarda pelo softmax
"""

from keras import backend as K

y_pred_encode = K.one_hot(K.argmax(y_pred), 4)
y_pred_encode

from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score

auc = roc_auc_score(Y_test, y_pred_encode)
accuracy = accuracy_score(Y_test, y_pred_encode)
recall = recall_score(Y_test, y_pred_encode, average='micro')
precision = precision_score(Y_test, y_pred_encode, average='micro')

print('AUC:%.3f'%auc)
print('accuracy:%.3f'%accuracy)
print('recall:%.3f'%recall)
print('precision:%.3f'%precision)

"""# Reamostragem

lendo novamente os dados par atratarmos eles desde o início
"""

X_train = pd.read_excel('')
Y_train = pd.read_csv('', header=None, names=['classes'])

X_test = pd.read_excel('')
Y_test = pd.read_csv('',header=None, names=['classes'])

dados_treino = pd.concat([X_train, Y_train], axis=1)

dados_treino['classes'].value_counts()

from sklearn.utils import resample

df_0=dados_treino[dados_treino['classes']==0]
df_1=dados_treino[dados_treino['classes']==1]
df_2=dados_treino[dados_treino['classes']==2]
df_3=dados_treino[dados_treino['classes']==3]

df_0_undersample=df_0.sample(n=20000,random_state=122)
df_1_upsample=resample(df_1,replace=True,n_samples=20000,random_state=123)
df_2_upsample=resample(df_2,replace=True,n_samples=20000,random_state=124)
df_3_upsample=resample(df_3,replace=True,n_samples=20000,random_state=125)

dados_treino_reamostrados=pd.concat([df_0_undersample,df_1_upsample,df_2_upsample,df_3_upsample])

dados_treino_reamostrados['classes'].value_counts()

"""separando novamente os dados"""

X_train_reamostrado = dados_treino_reamostrados.drop(['classes'], axis=1)
Y_train_reamostrado = dados_treino_reamostrados[['classes']]

X_train_reamostrado = X_train_reamostrado.values.reshape(len(X_train_reamostrado), X_train_reamostrado.shape[1],1)
X_test = X_test.values.reshape(len(X_test), X_test.shape[1],1)
Y_train_reamostrado = to_categorical(Y_train_reamostrado)
Y_test = to_categorical(Y_test)

"""Criando uma rede com os dados balanceados"""

model_balanceado,history_balanceado=network(X_train_reamostrado,Y_train_reamostrado,X_test,Y_test)

evaluate_model(history_balanceado,X_test,Y_test,model_balanceado)

y_pred_balanceado=model_balanceado.predict(X_test)

# Compute confusion matrix
cnf_matrix = confusion_matrix(Y_test.argmax(axis=1), y_pred_balanceado.argmax(axis=1))
np.set_printoptions(precision=2)

# Plot non-normalized confusion matrix
plt.figure(figsize=(15, 15))
plot_confusion_matrix(cnf_matrix, classes=['N', 'S', 'V', 'F'],normalize=False,
                      title='Matriz de confusão')
plt.rcParams.update({'font.size': 30})
plt.show()

y_pred_balanceado_encode = K.one_hot(K.argmax(y_pred_balanceado), 4)
y_pred_balanceado_encode

auc = roc_auc_score(Y_test, y_pred_balanceado_encode)
accuracy = accuracy_score(Y_test, y_pred_balanceado_encode)
recall = recall_score(Y_test, y_pred_balanceado_encode, average='micro')
precision = precision_score(Y_test, y_pred_balanceado_encode, average='micro')

print('AUC:%.3f'%auc)
print('accuracy:%.3f'%accuracy)
print('recall:%.3f'%recall)
print('precision:%.3f'%precision)
